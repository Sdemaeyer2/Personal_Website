---
title: 5 terms of Bayesian Stats you should understand
author: Sven De Maeyer
date: '2019-07-03'
slug: how-to-understand-bayesian-analyses
categories:
  - R
  - Bayesian
  - Statistics
tags:
  - Bayesian
subtitle: ''
summary: ''
authors: []
lastmod: '2019-07-03T22:29:21+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r}
library(tidyverse)
```

More and more in research we get confronted with colleagues that apply Bayesian analyses. They use a specific vocabulary: "posterior distribution", "Highest Probability Density Intervals", "priors", "MCMC", "burn-in", ... But what's the meaning of these terms? And how to interpret the essentials of Bayesian analyses? 

In this post I'll try to explain the basics and explain 5 key concepts. But before that, a short notice on the goal of stats (because sometimes we seem to forget).

# Stats? Why?

To put it simple: stats help us to model reality in numbers. Let's say we have a major concern on the rise of *vampires* in Europe ^[Ok. We can have serious doubts about whether this is the best example to demonstrate how we *model reality*]. The major question then is: how many vampires are out there? This is a statistical question that can be approached by a pretty simple *model*: a percentage of Vampires among the whole population of Europeans. This model only contains one unknown value - the actual percentage of Vampires - This unknown value is what we often call *a parameter*. In the exceptional case that we can test anyone of the whole European population we can calculate the exact percentage. But that's utopia. In research most of the time we make use of samples (we test a random choosen number of European citizens) and we do not calculate the exact parameter value. We *estimate* the parameter value. Actually statistics help us to estimate parameter values. But it does even more. Good statistics also *quantifies the (un)certainty* on our parameter estimates! If we apply this to our question concerning the percentage of Vampires in Europe, we want statistics to help us quantify how certain we are about different percentages of vampires as the true number in the population.

This brings us to our two first concepts from Bayesian analysis: priors and posteriors.

# Priors and posteriors

Bayesian statistics calls the quantification of uncertainty on parameter estimates <u>**priors and posteriors**</u>. So both concepts refer to some kind of probability. 

Lets start with the prior. 

> A prior is a quantification of (un)certainty on parameter estimates that we have before we consider the information in the data that we gather. 

If we apply this to our example of vampires, a prior quantifies our prior knowledge on the probability of each possible percentage of vampires in Europe. In this case this means that we, as researchers, allocate a probability to each possible percentage of vampires that this percentage is the true percentage in the European population. Now, imagine in this case that we do not have any clue before we start a research than each percentage has the same probability. The next plot shows our prior! Note that we have no data yet. So this graph visualizes our prior knowledge about the proportion of vampires.

```{r, echo = FALSE, warning = FALSE, error = FALSE}
set.seed(1975)

Vamp_data_weinig <- sample(c(TRUE, FALSE), prob = c(0.14, 0.86),
                   size = 20, replace = TRUE)
n_draws = 10000
prior_prop = c(1, 1)
library(tidyverse)
data <- as.logical(Vamp_data_weinig)
data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 20)))
proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
dens_curves <- map_dfr(data_indices, function(i) {
    value <- ifelse(i == 0, "Prior", ifelse(data[i], "Vampire", "Not a Vampire"))
    label <- paste0("n=", i)
    probability <- dbeta(proportion_success,
                         prior_prop[1] + sum(data[seq_len(i)]),
                         prior_prop[2] + sum(!data[seq_len(i)]))
    probability <- probability / max(probability)
    data_frame(value, label, proportion_success, probability)
  })

dens_curves$label <- fct_rev(factor(dens_curves$label, levels =  paste0("n=", data_indices )))

dens_curves$value <- factor(dens_curves$value, levels = c("Prior", "Vampire", "Not a Vampire"))

library(purrr)

dens_curves0 <- dens_curves %>% filter(value == "Prior") 

p <- ggplot(dens_curves0, aes(x = proportion_success, y = label,
                               height = probability, fill = value)) 
p +
    ggridges::geom_density_ridges(stat="identity", color = "white", alpha = 0.8,
                                  panel_scaling = TRUE, size = 1) +
    scale_y_discrete("", expand = c(0.01, 0)) +
    scale_x_continuous("Proportion of Vampires") +
    scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", drop = FALSE) +
    ggtitle(paste0(
      "Prior distribution (before gathering data (so n=0))")) +
    theme_light() +
    theme(legend.position = "top")
```

Of course, in research we gather **data**. In our example we're lucky that prof. dr. Van Helsing ^[Prof.dr. Van Helsing first appeared in the novel "Dracula". He's the vampire hunter.]. He developed the ultimate *Vampire Test* that determines whether a person is a vampire or not. With some sponsoring of the European Commission he was able to draw a random sample of 200 Europeans and applied his test. 

```{r, echo=F, warning=F, message=F, error=F,}

set.seed(1975)

Vamp_data_vol <- sample(c(TRUE, FALSE), prob = c(0.14, 0.86),
                   size = 200, replace = TRUE)

set.seed(1975)

Vamp_data_weinig <- sample(c(TRUE, FALSE), prob = c(0.14, 0.86),
                   size = 20, replace = TRUE)

set.seed(1975)

Vamp_data_zeerweinig <- sample(c(TRUE, FALSE), prob = c(0.14, 0.86),
                   size = 1, replace = TRUE)

prop_model <- function(data = c(), prior_prop = c(1, 1), n_draws = 10000) {
  library(tidyverse)
  data <- as.logical(data)
  # data_indices decides what densities to plot between the prior and the posterior
  # For 20 datapoints and less we're plotting all of them.
  data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 20)))
  
  # dens_curves will be a data frame with the x & y coordinates for the 
  # denities to plot where x = proportion_success and y = probability
  proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
  dens_curves <- map_dfr(data_indices, function(i) {
    value <- ifelse(i == 0, "Prior", ifelse(data[i], "Vampire", "Not a Vampire"))
    label <- paste0("n=", i)
    probability <- dbeta(proportion_success,
                         prior_prop[1] + sum(data[seq_len(i)]),
                         prior_prop[2] + sum(!data[seq_len(i)]))
    probability <- probability / max(probability)
    data_frame(value, label, proportion_success, probability)
  })
  # Turning label and value into factors with the right ordering for the plot
  dens_curves$label <- fct_rev(factor(dens_curves$label, levels =  paste0("n=", data_indices )))
  dens_curves$value <- factor(dens_curves$value, levels = c("Prior", "Vampire", "Not a Vampire"))
  
  p <- ggplot(dens_curves, aes(x = proportion_success, y = label,
                               height = probability, fill = value)) +
    ggridges::geom_density_ridges(stat="identity", color = "white", alpha = 0.8,
                                  panel_scaling = TRUE, size = 1) +
    scale_y_discrete("", expand = c(0.01, 0)) +
    scale_x_continuous("Proportion of Vampires") +
    scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", drop = FALSE,
                      labels =  c("Prior   ", "Vampire   ", "Not a Vampire   ")) +
    ggtitle(paste0(
      "Binomial model - Data: ", sum(data),  " Vampires, " , sum(!data), " Not Vampires")) +
    theme_light() +
    theme(legend.position = "top")
  print(p)
  
  # Returning a sample from the posterior distribution that can be further 
  # manipulated and inspected
  posterior_sample <- rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data))
  invisible(posterior_sample)
}
prop_model(Vamp_data_zeerweinig)
```


```{r}
prop_model(Vamp_data_weinig)
```

